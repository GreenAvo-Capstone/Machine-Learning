{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FV8Hv5Pykbwg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H15rckFFlKH4",
        "outputId": "6d4a9b4e-d543-4368-80bf-4af608ae664d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Machine-Learning'...\n",
            "remote: Enumerating objects: 15057, done.\u001b[K\n",
            "remote: Counting objects: 100% (318/318), done.\u001b[K\n",
            "remote: Compressing objects: 100% (298/298), done.\u001b[K\n",
            "remote: Total 15057 (delta 18), reused 307 (delta 13), pack-reused 14739\u001b[K\n",
            "Receiving objects: 100% (15057/15057), 412.15 MiB | 15.57 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n",
            "Updating files: 100% (29426/29426), done.\n"
          ]
        }
      ],
      "source": [
        "# Menggunakan tanda seru (!) untuk menjalankan perintah shell\n",
        "!git clone https://github.com/GreenAvo-Capstone/Machine-Learning.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVklIgUdlNLk",
        "outputId": "9b1961e0-358b-4ced-8e1d-2baf3d330857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset berhasil diunduh.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Cek apakah direktori dataset sudah ada\n",
        "dataset_dir = './Machine-Learning/dataset'\n",
        "if os.path.exists(dataset_dir):\n",
        "    print(\"Dataset berhasil diunduh.\")\n",
        "else:\n",
        "    print(\"Dataset tidak ditemukan.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5vfGAtzfk9WT"
      },
      "outputs": [],
      "source": [
        "path_train = os.path.join(dataset_dir, 'train')\n",
        "path_val = os.path.join(dataset_dir, 'val')\n",
        "path_test = os.path.join(dataset_dir, 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qo2A_Wnjk9Yw"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "img_size = 224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl2cTj9Pk9bG",
        "outputId": "bebccc17-ef56-47c7-cf17-e0d38b981ebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11032 files belonging to 5 classes.\n",
            "Found 2205 files belonging to 5 classes.\n",
            "Found 1473 files belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(path_train, shuffle=True, batch_size = BATCH_SIZE, image_size = (img_size, img_size), label_mode = \"categorical\")\n",
        "valid_ds = tf.keras.utils.image_dataset_from_directory(path_val, shuffle=True, batch_size = BATCH_SIZE, image_size = (img_size, img_size), label_mode = \"categorical\")\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(path_test, shuffle=False, batch_size = BATCH_SIZE, image_size = (img_size, img_size), label_mode = \"categorical\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear previous sessions\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Define image size\n",
        "img_size = 224\n",
        "\n",
        "# Load datasets\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    path_train,\n",
        "    image_size=(img_size, img_size),\n",
        "    batch_size=32,\n",
        "    label_mode='int'\n",
        ")\n",
        "\n",
        "valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    path_val,\n",
        "    image_size=(img_size, img_size),\n",
        "    batch_size=32,\n",
        "    label_mode='int'\n",
        ")\n",
        "\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    path_test,\n",
        "    image_size=(img_size, img_size),\n",
        "    batch_size=32,\n",
        "    label_mode='int'\n",
        ")\n",
        "\n",
        "# Get class names from training dataset\n",
        "class_names = train_ds.class_names\n",
        "num_class = len(class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c7UKhLfG3VC",
        "outputId": "6e248750-1e9c-4369-ade1-2bbd35dc3f4e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11032 files belonging to 5 classes.\n",
            "Found 2205 files belonging to 5 classes.\n",
            "Found 1473 files belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply additional preprocessing if needed\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.resize(image, [img_size, img_size])\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "    train_ds = train_ds.map(preprocess).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    valid_ds = valid_ds.map(preprocess).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    test_ds = test_ds.map(preprocess).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Check the shapes of a batch of images and labels\n",
        "for images, labels in train_ds.take(1):\n",
        "    print(f\"Images shape: {images.shape}\")\n",
        "    print(f\"Labels shape: {labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROhTauzWI1w7",
        "outputId": "a21fe98f-aab2-420f-fe4e-5ba342dee016"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images shape: (32, 224, 224, 3)\n",
            "Labels shape: (32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQP1lh5Xk9da",
        "outputId": "5054e83e-47f4-4c08-88cb-8ddf7b68739d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n",
            "12683000/12683000 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load MobileNetV3Large with pre-trained weights and make it trainable\n",
        "base_model = tf.keras.applications.MobileNetV3Large(input_shape=(img_size, img_size, 3), include_top=False, pooling=\"max\")\n",
        "base_model.trainable = True\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input((img_size, img_size, 3), name=\"Input\", dtype=tf.float32),\n",
        "    tf.keras.layers.Rescaling(scale=1./255, name=\"Rescale_Layer\"),\n",
        "    base_model,\n",
        "    tf.keras.layers.Dense(128, activation='selu', kernel_initializer=\"lecun_normal\", name=\"FC\"),\n",
        "    tf.keras.layers.Dense(num_class, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),\n",
        "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1, name='precision'),\n",
        "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1, name='recall')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define callbacks\n",
        "bestCB = tf.keras.callbacks.ModelCheckpoint(filepath=f'best_model.h5', verbose=1, save_best_only=True)\n",
        "lrCB = tf.keras.callbacks.ReduceLROnPlateau(verbose=1, min_lr=1e-20, patience=5)\n",
        "esCB = tf.keras.callbacks.EarlyStopping(patience=45, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3pej2NSk9fv"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data = valid_ds,\n",
        "    epochs = 50,\n",
        "    callbacks=[bestCB, lrCB, esCB]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QSi7Qm3k9kk"
      },
      "outputs": [],
      "source": [
        "model.save(\"./checkpoint/latest/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label = \"Training Loss\")\n",
        "plt.plot(history.history['val_loss'], label = \"Validation Loss\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"Loss History\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['precision'], label = \"Training Precision\")\n",
        "plt.plot(history.history['val_precision'], label = \"Validation Precision\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"Precision History\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zltQhN46jMcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhfFN6QCk9nE"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = history.epoch\n",
        "\n",
        "plt.plot(epochs, acc, label='Train Accuracy')\n",
        "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model's Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs[10:], loss[10:], label='Train Loss')\n",
        "plt.plot(epochs[10:], val_loss[10:], label='Validation Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Model's Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lw9tstsck9p4"
      },
      "outputs": [],
      "source": [
        "# Load iterasi terakhir\n",
        "# model = tf.keras.models.load_model(\"./checkpoint/latest/\")\n",
        "\n",
        "# Load terbaik\n",
        "best_model = tf.keras.models.load_model(f'best_model.h5')\n",
        "\n",
        "# Evaluasi model pada data testing\n",
        "loss, accuracy, precision, recall = best_model.evaluate(test_ds)\n",
        "print(\"Loss pada data testing:\", loss)\n",
        "print(\"Akurasi pada data testing:\", accuracy)\n",
        "print(\"Precision pada data testing:\", precision)\n",
        "print(\"Recall pada data testing:\", recall)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediksi kelas untuk setiap gambar di data testing\n",
        "predictions = best_model.predict(test_ds)\n",
        "predicted_classes = tf.math.argmax(predictions, axis=1)"
      ],
      "metadata": {
        "id": "4GajH9YIHPJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tampilkan beberapa contoh prediksi\n",
        "for images, labels in test_ds.take(1):\n",
        "    for i in range(5):  # Tampilkan 5 gambar pertama\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(f\"Prediksi: {class_names[predicted_classes[i]]}, \\\n",
        "                  Aktual: {class_names[labels[i]]}\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "k8TAYCieHPLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan Model\n",
        "model.save('model_alpukat.h5')"
      ],
      "metadata": {
        "id": "8RBQnZ0g1Cfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KSjZtNHQ00Oe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert the Model to TensorFlow Lite"
      ],
      "metadata": {
        "id": "lIwlrH7n07cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "zzT45l-XNm9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load model .h5\n",
        "model = tf.keras.models.load_model('/content/best_model.h5')\n",
        "\n",
        "# Konversi model ke format TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Simpan model TFLite ke file\n",
        "with open('Avocado Ripeness Model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "RtUFxOH8RVJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0j-geOkkRVQs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}