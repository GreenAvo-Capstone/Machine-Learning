{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj7-FNWnEMkH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "os.environ[\"TF_GPU_THREAD_MODE\"] = \"gpu_private\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Flatten, Activation, BatchNormalization, Rescaling\n",
        "from tensorflow.keras.layers import RandomContrast, RandomZoom, RandomFlip, RandomRotation, RandomTranslation, RandomCrop, RandomBrightness, GaussianNoise\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback, EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix, RocCurveDisplay, ConfusionMatrixDisplay\n",
        "from absl import logging\n",
        "\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "np.set_printoptions(suppress=True)\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6YHD9O5Hv54"
      },
      "source": [
        "# Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj6ESun2FWsa",
        "outputId": "f8402b6e-017c-40fe-820a-751f27a03330"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Machine-Learning'...\n",
            "remote: Enumerating objects: 15048, done.\u001b[K\n",
            "remote: Counting objects: 100% (309/309), done.\u001b[K\n",
            "remote: Compressing objects: 100% (289/289), done.\u001b[K\n",
            "remote: Total 15048 (delta 14), reused 307 (delta 13), pack-reused 14739\u001b[K\n",
            "Receiving objects: 100% (15048/15048), 412.13 MiB | 18.07 MiB/s, done.\n",
            "Resolving deltas: 100% (25/25), done.\n",
            "Updating files: 100% (29424/29424), done.\n"
          ]
        }
      ],
      "source": [
        "# Menggunakan tanda seru (!) untuk menjalankan perintah shell\n",
        "!git clone https://github.com/GreenAvo-Capstone/Machine-Learning.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28ah7KMbFWv2",
        "outputId": "20919c80-a24e-44aa-ad21-f047a44cddeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset berhasil diunduh.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Cek apakah direktori dataset sudah ada\n",
        "dataset_dir = './Machine-Learning/dataset'\n",
        "if os.path.exists(dataset_dir):\n",
        "    print(\"Dataset berhasil diunduh.\")\n",
        "else:\n",
        "    print(\"Dataset tidak ditemukan.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp47IwYUEUOf",
        "outputId": "9f211d89-dc81-45db-a755-bc4c86ff6142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11032 files belonging to 5 classes.\n",
            "Found 2205 files belonging to 5 classes.\n",
            "Found 1473 files belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "classes_name = os.listdir(dataset_dir)\n",
        "dict_disease = {}\n",
        "\n",
        "for number, name in enumerate(classes_name):\n",
        "    if name == 'filename': continue\n",
        "    dict_disease[number] = name.strip()\n",
        "dict_disease = {key: dict_disease[key] for key in dict_disease if key != 'Unlabeled'}\n",
        "img_size = 224\n",
        "\n",
        "path_train = os.path.join(dataset_dir, 'train')\n",
        "path_valid = os.path.join(dataset_dir, 'val')\n",
        "path_test = os.path.join(dataset_dir, 'test')\n",
        "\n",
        "# Memeriksa apakah direktori valid dan test ada\n",
        "if not os.path.exists(path_train):\n",
        "    raise FileNotFoundError(f\"Training directory not found: {path_train}\")\n",
        "if not os.path.exists(path_valid):\n",
        "    raise FileNotFoundError(f\"Validation directory not found: {path_valid}\")\n",
        "if not os.path.exists(path_test):\n",
        "    raise FileNotFoundError(f\"Test directory not found: {path_test}\")\n",
        "\n",
        "# Mengatur AUTOTUNE\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# **Dynamically determine classes:**\n",
        "all_classes = set()\n",
        "for dataset_path in [path_train, path_valid, path_test]:\n",
        "    for class_name in os.listdir(dataset_path):\n",
        "        if class_name != 'filename':\n",
        "            all_classes.add(class_name)\n",
        "\n",
        "dict_disease = {i: class_name for i, class_name in enumerate(sorted(all_classes))}\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "    train_dataset = tf.keras.utils.image_dataset_from_directory(path_train, shuffle=False, batch_size=None, image_size=(img_size, img_size))\n",
        "    valid_dataset = tf.keras.utils.image_dataset_from_directory(path_valid, shuffle=False, batch_size=None, image_size=(img_size, img_size))\n",
        "    test_dataset = tf.keras.utils.image_dataset_from_directory(path_test, shuffle=False, batch_size=None, image_size=(img_size, img_size))\n",
        "\n",
        "    test_labels = []\n",
        "    for _, label in test_dataset:\n",
        "        test_labels.append(label.numpy())\n",
        "    test_labels = np.array(test_labels).reshape(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kan53yvEURF"
      },
      "outputs": [],
      "source": [
        "with tf.device('/cpu:0'):\n",
        "    train = train_dataset.cache('train_dataset').repeat(1).shuffle(256, reshuffle_each_iteration=True).batch(32).prefetch(AUTOTUNE)\n",
        "    valid = valid_dataset.cache('valid-dataset').batch(32).prefetch(AUTOTUNE)\n",
        "    test = test_dataset.cache('test-dataset').batch(32).prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bkx77VGSeW5"
      },
      "outputs": [],
      "source": [
        "def confusionMatrix(epoch, logs):\n",
        "    test_loss, test_acc, topK_acc = model.evaluate(test, verbose=0)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\\nTest Accuracy: {test_acc*100:.2f}%\\nTopK Accuracy: {topK_acc*100:.2f}%\")\n",
        "\n",
        "    yhat = model.predict(test, verbose=0)\n",
        "    yhat = np.argmax(yhat, axis=1)\n",
        "\n",
        "    # Check the number of unique labels in the test dataset\n",
        "    unique_labels = np.unique(test_labels)\n",
        "    cm_labels = [dict_disease[i] for i in unique_labels]\n",
        "\n",
        "    # Plotting confusion matrix\n",
        "    disp = ConfusionMatrixDisplay.from_predictions(\n",
        "        test_labels,\n",
        "        yhat,\n",
        "        display_labels=cm_labels,\n",
        "        xticks_rotation=45\n",
        "    )\n",
        "\n",
        "    # Adjust the number of tick locations\n",
        "    disp.ax_.set_xticks(np.arange(len(cm_labels)))\n",
        "    disp.ax_.set_xticklabels(cm_labels, rotation=45)\n",
        "    disp.ax_.set_yticks(np.arange(len(cm_labels)))\n",
        "    disp.ax_.set_yticklabels(cm_labels)\n",
        "\n",
        "    plt.title(f\"Confusion Matrix on epoch {epoch+1}\\nVal accuracy: {logs.get('val_accuracy'):.3f}\\nTest accuracy: {test_acc:.3f}\")\n",
        "\n",
        "    try:\n",
        "        os.makedirs(\"./CM/\")\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "\n",
        "    plt.savefig(f\"./CM/Epoch {epoch+1}.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close('all')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIaYUcoFHl7a"
      },
      "source": [
        "# Making the Machine Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2v0oL4PEUV6"
      },
      "outputs": [],
      "source": [
        "base_model = tf.keras.applications.MobileNetV3Large(input_shape=(img_size,img_size,3), include_top=False, include_preprocessing=False, pooling='max')\n",
        "base_model.trainable=False\n",
        "\n",
        "model = Sequential([\n",
        "    Input((img_size, img_size, 3), dtype=tf.float32),\n",
        "    GaussianNoise(stddev=0.5),\n",
        "    Rescaling(1/255),\n",
        "    RandomFlip(),\n",
        "    RandomRotation(factor=0.3),\n",
        "    RandomZoom(height_factor=(-0.2, 0.2)),\n",
        "    RandomBrightness(factor=(-0.2,0.2), value_range=[0,1]),\n",
        "    base_model,\n",
        "    Dense(128, activation='selu'),\n",
        "    # Make sure the final Dense layer matches the number of classes:\n",
        "    Dense(len(dict_disease))\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, amsgrad=True),\n",
        "    metrics=['accuracy', tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3)]\n",
        ")\n",
        "\n",
        "bestCB = ModelCheckpoint(filepath='./checkpoint/best/', monitor='val_accuracy',\n",
        "                         mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "cmCB = LambdaCallback(on_epoch_end=confusionMatrix)\n",
        "\n",
        "esCB = EarlyStopping(monitor='val_accuracy', patience=50, restore_best_weights=True)\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJBhmli6H5oY"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02vHQVPaEUan"
      },
      "outputs": [],
      "source": [
        "with tf.device('/gpu:0'):\n",
        "    history = model.fit(\n",
        "        train,\n",
        "        validation_data=valid,\n",
        "        epochs=1000,\n",
        "        callbacks=[bestCB, cmCB, esCB]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXPtimxeEUc3"
      },
      "outputs": [],
      "source": [
        "model.save(\"./checkpoint/latest/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOvf-uyOEUfT"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "topK_acc = history.history['sparse_top_k_categorical_accuracy']\n",
        "val_topK_acc = history.history['val_sparse_top_k_categorical_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = history.epoch\n",
        "\n",
        "plt.plot(epochs, acc, label='Train Accuracy')\n",
        "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model's Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, topK_acc, label='Train TopK Accuracy')\n",
        "plt.plot(epochs, val_topK_acc, label='Validation TopK Accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"TopK Accuracy\")\n",
        "plt.title(\"Model's TopK Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs[10:], loss[10:], label='Train Loss')\n",
        "plt.plot(epochs[10:], val_loss[10:], label='Validation Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Model's Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x153xtGGICiD"
      },
      "source": [
        "# Evaluate the Model Created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bep0By1VEUhj"
      },
      "outputs": [],
      "source": [
        "# Load iterasi terakhir\n",
        "# model = tf.keras.models.load_model(\"./checkpoint/latest/\")\n",
        "\n",
        "# Load terbaik\n",
        "model = tf.keras.models.load_model(\"./checkpoint/best/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbcWLkcEIFSV"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc, topK_acc = model.evaluate(test, verbose=0)\n",
        "print(f\"Test Loss: {test_loss:.4f}\\nTest Accuracy: {test_acc*100:.2f}%\\nTopK Accuracy: {topK_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4feKeygJEUk8"
      },
      "outputs": [],
      "source": [
        "topK = 3 # Ambil 3 kategori tertinggi untuk display ke user\n",
        "\n",
        "for number, (image, label) in enumerate(test_dataset.shuffle(512).take(20)):\n",
        "    image = tf.cast(image, tf.uint8)\n",
        "\n",
        "    plt.imshow(image, interpolation='nearest')\n",
        "    plt.title(dict_disease[label.numpy()])\n",
        "    plt.axis('off')\n",
        "\n",
        "    pred_image      = tf.cast(image, tf.float32)\n",
        "    pred_image      = tf.expand_dims(image, axis=0)\n",
        "    yhat            = tf.nn.softmax(model.predict(pred_image, verbose=0))\n",
        "    yhat_topk       = np.argpartition(-yhat, topK-1)[0][:3]\n",
        "    true_label      = dict_disease[label.numpy()]\n",
        "    prob_res        = []\n",
        "    disease_types   = []\n",
        "    text_result     = []\n",
        "\n",
        "    for disease in yhat_topk:\n",
        "        disease_types.append(dict_disease[disease])\n",
        "        prob_res.append(yhat[0][disease])\n",
        "\n",
        "    for index_disease, disease in enumerate(disease_types):\n",
        "        res = \"benar\" if true_label == disease else \"salah\"\n",
        "        res = f\"{index_disease+1}: {disease} ({res}), {prob_res[index_disease] * 100:.2f}%.\"\n",
        "        text_result.append(res)\n",
        "\n",
        "    plt.figtext(0.5, 0.01, \"\\n\".join(text_result), ha='center', wrap=True)\n",
        "    plt.savefig(f\"Prediction {number+1}.png\", transparent=True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNpT9VSdIGZt"
      },
      "source": [
        "# Convert the Model to TensorFlow Lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRBT_86XII2w"
      },
      "outputs": [],
      "source": [
        "# Optimize size\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"./checkpoint/best/\")\n",
        "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_file = pathlib.Path('model size.tflite')\n",
        "tflite_file.write_bytes(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Hmzucl8IJqS"
      },
      "outputs": [],
      "source": [
        "# Optimize latency\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"./checkpoint/best/\")\n",
        "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_file = pathlib.Path('model latency.tflite')\n",
        "tflite_file.write_bytes(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OObl3-riYz8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}